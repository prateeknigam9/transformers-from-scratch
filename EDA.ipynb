{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_450qu70wv6q"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/datasets/stefanlarson/outofscope-intent-classification-dataset?resource=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RNySDu2pwv6s"
      },
      "outputs": [],
      "source": [
        "# import kagglehub\n",
        "\n",
        "# kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evmT90sgwv6t",
        "outputId": "4e04a924-e13a-42e5-99b3-0b2a61896274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\nigam\\.cache\\kagglehub\\datasets\\stefanlarson\\outofscope-intent-classification-dataset\\versions\\1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"stefanlarson/outofscope-intent-classification-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri3xldUkw8JJ",
        "outputId": "236f7324-842b-46da-98f0-269488d4a04a"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jusHNpdkwv6u"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CkzMcxOJwv6v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, random_split\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from model_blocks import *\n",
        "from transformer import EncoderOnlyTransformer\n",
        "from pathlib import Path\n",
        "import torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import utils "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS0kuvdLwv6v"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM5ed027wv6v",
        "outputId": "74854ab3-b2fb-4507-8e47-a91f9ecdecb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssaSmkhOwv6w",
        "outputId": "d006427c-4515-4051-f11a-f09c2b542b15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x211689368d0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIqimY_cwv6w"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcgRZVD1wv6x",
        "outputId": "43e77e26-e100-482a-e387-10d4c6d55221"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is_test.json',\n",
              " 'is_train.json',\n",
              " 'is_val.json',\n",
              " 'oos_test.json',\n",
              " 'oos_train.json',\n",
              " 'oos_val.json']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join(path, 'is_train.json'), 'r') as file:\n",
        "    train_data = json.load(file)\n",
        "with open(os.path.join(path, 'is_val.json'), 'r') as file:\n",
        "    val_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaEdcoglwv6x",
        "outputId": "4e6a6951-0ade-4e72-ab10-59dd8cf8c4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data:\n",
            "['what expression would i use to say i love you if i were an italian', 'translate']\n",
            "what expression would i use to say i love you if i were an italian\n",
            "translate\n",
            "\n",
            " validation data:\n",
            "['in spanish, meet me tomorrow is said how', 'translate']\n",
            "in spanish, meet me tomorrow is said how\n",
            "translate\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(train_data)):\n",
        "    print(\"Train data:\")\n",
        "    print(train_data[i])\n",
        "    print(train_data[i][0])\n",
        "    print(train_data[i][1])\n",
        "    print(\"\\n validation data:\")\n",
        "    print(val_data[i])\n",
        "    print(val_data[i][0])\n",
        "    print(val_data[i][1])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_preprocessing(ds):\n",
        "    prompt = []\n",
        "    label = []\n",
        "    ds_upd = pd.DataFrame()\n",
        "    \n",
        "    for indiv_data in ds:\n",
        "        prompt.append(indiv_data[0])\n",
        "        label.append(indiv_data[1])\n",
        "    ds_upd[\"prompt\"] = prompt\n",
        "    ds_upd[\"label_str\"] = label\n",
        "    \n",
        "    unique_classes = ds_upd['label_str'].unique()\n",
        "    class_to_id = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
        "    ds_upd['label'] = ds_upd['label_str'].map(class_to_id)\n",
        "    \n",
        "    return ds_upd, unique_classes\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data, unique_classes = data_preprocessing(train_data)\n",
        "val_data, unique_classes = data_preprocessing(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>label_str</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9151</th>\n",
              "      <td>will you let me know what my spending limit is</td>\n",
              "      <td>credit_limit</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1713</th>\n",
              "      <td>heads or tails i choose tails</td>\n",
              "      <td>flip_coin</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14544</th>\n",
              "      <td>can you tell me my current salary</td>\n",
              "      <td>income</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6758</th>\n",
              "      <td>how long can pizza be in the fridge</td>\n",
              "      <td>food_last</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8828</th>\n",
              "      <td>what is the next holiday on the calendar</td>\n",
              "      <td>next_holiday</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               prompt     label_str  label\n",
              "9151   will you let me know what my spending limit is  credit_limit     91\n",
              "1713                    heads or tails i choose tails     flip_coin     17\n",
              "14544               can you tell me my current salary        income    145\n",
              "6758              how long can pizza be in the fridge     food_last     67\n",
              "8828         what is the next holiday on the calendar  next_holiday     88"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ngrS2W0wv6y"
      },
      "source": [
        "#  Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4130, 4491, 2252, 5152, 5286, 579, 4190]\n",
            "class fair, chat, persons jfk disney trader\n"
          ]
        }
      ],
      "source": [
        "corpus = \" \".join(train_data[\"prompt\"])\n",
        "tokenizer = utils.selfTokenizer(corpus)\n",
        "print(tokenizer.encode(\"i need you to translate the sentence\"))\n",
        "print(tokenizer.decode([5140, 5500, 539, 3868, 420, 3175, 2041]))\n",
        "del tokenizer, corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could use our own self made tokenizer class, hwoever GPT-2 uses BytePair encoding as tokenizer, and we would follow the same, as it allows to handle out-of-vocab words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i need you to translate the sentence'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = utils.get_or_build_tokenizer(train_data)\n",
        "tokenizer.encode(\"i need you to translate the sentence\").ids\n",
        "tokenizer.decode([4, 21, 9, 6, 646, 7, 2704])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[72, 761, 345, 284, 15772, 262, 6827]\n",
            "i need you to translate the sentence\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer.encode(\"i need you to translate the sentence\",allowed_special=\"all\"))\n",
        "print(tokenizer.decode([72, 761, 345, 284, 15772, 262, 6827]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "136\n"
          ]
        }
      ],
      "source": [
        "max_seq_len = len(max(train_data['prompt'],key=len))\n",
        "print(max_seq_len)\n",
        "train_data = utils.promptDataset(tokenizer,train_data,max_seq_len+1)\n",
        "val_data = utils.promptDataset(tokenizer,val_data,max_seq_len+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=10,shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(train_data,batch_size=2,shuffle=False)\n",
        "# data_iter = iter(train_loader)\n",
        "# inputs, targets = next(data_iter)\n",
        "# print(\"Inputs:\\n\", inputs)\n",
        "# print(\"\\nTargets:\\n\", targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYkI5q0pwv6z"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pmmLSKBlwv60"
      },
      "outputs": [],
      "source": [
        "def build_encoder_only_transformer(src_vocab_size, n_classes,\n",
        "                                   seq_len, embedding_dim=512,\n",
        "                                   N =6, h=8, dropout=0.01, d_ff=2048):\n",
        "\n",
        "    src_embed = InputEmbeddings(src_vocab_size,embedding_dim)\n",
        "    src_pos = PositionalEncoding(embedding_dim, seq_len, dropout)\n",
        "\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        self_attention_block = MultiHeadAttention(seq_len, embedding_dim, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_ff,embedding_dim, dropout)\n",
        "        enc = encoder_block(self_attention_block,feed_forward_block,dropout)\n",
        "        encoder_blocks.append(enc)\n",
        "\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    transformer = EncoderOnlyTransformer(src_embed, src_pos, encoder,\n",
        "                                         n_classes, src_vocab_size,\n",
        "                                         embedding_dim)\n",
        "\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Yyj3KgbRwv60"
      },
      "outputs": [],
      "source": [
        "def get_model(tokenizer,n_classes):\n",
        "    model = build_encoder_only_transformer(tokenizer.max_token_value, n_classes, max_seq_len)\n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8kjbYgp2MeI",
        "outputId": "b76d98e2-e92c-4715-f981-bd206d20c619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150\n"
          ]
        }
      ],
      "source": [
        "print(len(unique_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'bool' object has no attribute 'sum'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
          ]
        }
      ],
      "source": [
        "([1,0,0] == [0,0,0]).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "eWit9J6Owv60"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model,dataloader, device=device):\n",
        "    losses, acc, count = [], 0, 0\n",
        "    batch_iterator = tqdm(enumerate(dataloader),total=len(dataloader))\n",
        "    for idx, batch in batch_iterator:\n",
        "        encoder_input = batch[0].to(device)\n",
        "        label = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model.encode(encoder_input,None).to(device)\n",
        "        # output = output[:, -1, :] # Get predictions for the last token\n",
        "        loss = loss_fn(output, label).to(device)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # global_step += 1\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        predicted_class = torch.argmax(output, dim=-1)\n",
        "\n",
        "        batch_correct = (predicted_class == label).sum().item()\n",
        "        batch_accuracy = batch_correct / len(label)\n",
        "        acc += batch_correct\n",
        "        count += len(label)\n",
        "\n",
        "        batch_iterator.set_postfix({\"train_loss\": f\"{loss.item():6.3f}\",\n",
        "                                     \"batch_acc\": f\"{batch_accuracy:.4f}\",\n",
        "                                     \"train_acc\": f\"{acc/count:.4f}\"})\n",
        "\n",
        "    return np.mean(losses), acc/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "oFR7q1i-CvMb"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_data_batch, device=device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_data_batch:\n",
        "            encoder_input = batch[0].to(device)\n",
        "            label = batch[1].to(device)\n",
        "            output = model.encode(encoder_input, None)\n",
        "\n",
        "            # import pdb; pdb.set_trace()\n",
        "            # output = output[:, -1, :]  # Get predictions for the last token\n",
        "            loss = loss_fn(output, label).to(device)\n",
        "\n",
        "            predicted_class = torch.argmax(output, dim=-1)\n",
        "            batch_correct = (predicted_class == label).sum().item()\n",
        "            batch_accuracy = batch_correct / len(label)\n",
        "            acc = batch_correct\n",
        "            count = len(label)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "    return np.mean(losses), acc/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "28P00sv8CnVe"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, test_loader, epochs):\n",
        "    for ep in range(epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader)\n",
        "        val_loss, val_acc = evaluate_model(model, test_loader)\n",
        "        print(f'ep {ep}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxDNmOWcEkvO",
        "outputId": "2e61b540-175b-4e5e-e107-fa5f8a451720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "EncoderOnlyTransformer                                  --\n",
              "├─InputEmbeddings: 1-1                                  --\n",
              "│    └─Embedding: 2-1                                   25,731,072\n",
              "├─PositionalEncoding: 1-2                               --\n",
              "│    └─Dropout: 2-2                                     --\n",
              "├─Encoder: 1-3                                          --\n",
              "│    └─ModuleList: 2-3                                  --\n",
              "│    │    └─encoder_block: 3-1                          3,150,342\n",
              "│    │    └─encoder_block: 3-2                          3,150,342\n",
              "│    │    └─encoder_block: 3-3                          3,150,342\n",
              "│    │    └─encoder_block: 3-4                          3,150,342\n",
              "│    │    └─encoder_block: 3-5                          3,150,342\n",
              "│    │    └─encoder_block: 3-6                          3,150,342\n",
              "├─Linear: 1-4                                           76,950\n",
              "================================================================================\n",
              "Total params: 44,710,074\n",
              "Trainable params: 44,710,074\n",
              "Non-trainable params: 0\n",
              "================================================================================"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = get_model(tokenizer,n_classes=len(unique_classes))\n",
        "torchinfo.summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "D0XsBmB-Esey"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=10**-4,eps= 1e-9)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo4eYeDtEDUo",
        "outputId": "747bdfc1-3c6c-44f7-a523-55712e2947a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1500 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[51], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, train_loader, test_loader, epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 3\u001b[0m         train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[49], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# output = output[:, -1, :] # Get predictions for the last token\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, label)\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\nigam\\OneDrive\\Documents\\self\\classification\\transformer.py:21\u001b[0m, in \u001b[0;36mEncoderOnlyTransformer.encode\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos(x)\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, src_mask)\n",
            "File \u001b[1;32mc:\\Users\\nigam\\anaconda3\\envs\\ml_dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nigam\\anaconda3\\envs\\ml_dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nigam\\OneDrive\\Documents\\self\\classification\\common_blocks.py:11\u001b[0m, in \u001b[0;36mInputEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim)\n",
            "File \u001b[1;32mc:\\Users\\nigam\\anaconda3\\envs\\ml_dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nigam\\anaconda3\\envs\\ml_dev\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nigam\\anaconda3\\envs\\ml_dev\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nigam\\anaconda3\\envs\\ml_dev\\Lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "train(model, train_loader, val_loader, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hltkVhL1wv60"
      },
      "outputs": [],
      "source": [
        "def predict_labels(model, tokenizer, prompts, class_to_id):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for prompt in prompts:\n",
        "            enc_input_tokens = tokenizer.encode(prompt).ids\n",
        "            encoder_input = torch.tensor(enc_input_tokens, dtype=torch.int64).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model.encode(encoder_input, None)\n",
        "            # output = output[:, -1, :]  # Get predictions for the last token\n",
        "            predicted_class = torch.argmax(output, dim=-1).item()\n",
        "            predictions.append(predicted_class)\n",
        "\n",
        "    # Map predicted classes back to text labels\n",
        "    id_to_class = {idx: cls for cls, idx in class_to_id.items()}\n",
        "    predicted_labels = [id_to_class[pred] for pred in predictions]\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_GoUQk5wv61"
      },
      "outputs": [],
      "source": [
        "def get_sample_prompts_and_labels(ds_upd, sample_size):\n",
        "\n",
        "    sampled_data = ds_upd.sample(n=sample_size)\n",
        "\n",
        "    prompts = sampled_data['prompt'].tolist()\n",
        "    actual_labels = sampled_data['completion'].map(lambda x: {v: k for k, v in class_to_id.items()}[x]).tolist()\n",
        "\n",
        "    return prompts, actual_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcVh4QNYwv61"
      },
      "outputs": [],
      "source": [
        "sample_size = 5\n",
        "\n",
        "sample_prompts, actual_labels = get_sample_prompts_and_labels(ds_upd, sample_size)\n",
        "\n",
        "predicted_labels = predict_labels(model, tokenizer, sample_prompts, class_to_id)\n",
        "\n",
        "for prompt, actual, predicted in zip(sample_prompts, actual_labels, predicted_labels):\n",
        "    print(f\"Prompt: {prompt}\\nActual Label: {actual}\\nPredicted Label: {predicted}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
